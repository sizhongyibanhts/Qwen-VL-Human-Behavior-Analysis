# 营业厅人员行为分析工具

## 1. 工具简介
business_hall_analysis：仅具备识别人员功能，可参考 https://zhuanlan.zhihu.com/p/1995169948964759155

### 1.1 原有方法
behavior_detection.py，基于多模态大模型的“文本描述→违规推导”流程，先整体分析工位/人员行为（输出自然语言描述），再基于描述文本推导违规行为及坐标，核心依赖大模型的自然语言理解与生成能力。

### 1.2 优化后方法
behavior_detection_sahi.py,参考目标检测中的sahi的处理思路，基于“单人员独立识别+坐标位置计算”的流程，先对每个人员独立提取特征与坐标，通过坐标运算判断位置关系，再结合规则判定违规，减少大模型主观判断依赖。

每个人员单独调用一次大模型，针对顾客与员工之间的互动、员工之间的互动，也分别截图调用大模型判断，避免模型在一整张图中识别单个人员行为，提升判断的准确度。

利用坐标进行位置关系判断（例如判断人员是否位于柜台内，进而判断身份）。

### 1.3 优化后方法的核心优势
1. **单人员独立识别**：避免多人员整体描述导致的特征混淆、信息遗漏，每个人员的行为/特征可精准追溯；
2. **坐标化位置计算**：用坐标运算替代大模型对“柜台内外/顾客位置”的主观判断，彻底解决位置关系判定的“幻觉问题”；
3. **规则化违规判定**：违规逻辑与大模型解耦，判定标准统一、结果可复现，降低对大模型生成质量的依赖。

优化后方法通过“单人员独立识别+坐标位置计算”的核心设计，解决了原有方法中“大模型文本幻觉、位置判断失真、人员特征混淆”的核心问题，检测结果的精准度、稳定性、可解释性均显著优于原有方法。

## 2.原有方法 核心流程

参见behavior_detection.py，大致流程如下：

```
输入图片 → 编码为Base64 → 分阶段分析 → 违规检测 → 可视化标注
```
- **阶段1（工位分析）**：调用多模态模型识别柜台内工位总数、每个工位位置/人员状态，输出文本描述并保存；
- **阶段2（人员行为分析）**：识别图片总人数、柜台位置，逐人分析身份/位置/衣着/手机使用/聊天/睡觉等行为，输出文本描述并保存；
- **阶段3（工位违规检测）**：基于工位描述，检测“岗位无人”违规，输出带坐标的JSON结果；
- **阶段3.5（人员描述简化）**：剔除顾客信息，仅保留工作人员的位置/身份/行为/潜在违规；
- **阶段4（人员违规检测）**：基于简化后的人员描述，检测“玩手机/聊天/睡觉/便服”违规，输出带坐标的JSON结果；
- **阶段5（可视化）**：读取违规JSON，在图片上绘制不同颜色的检测框+标签，完成标注。

## 3. 优化后方法 核心流程

参见behavior_detection_sahi.py，大致流程如下：

```
输入图片 → 编码为Base64 → 单人员独立识别 → 坐标化位置计算 → 违规判定 → 可视化标注
```
- **阶段1（识别办公设备和工作台）**：使用视觉大模型检测图像中的显示器、键盘和办公桌，输出其归一化坐标框（0–999范围），以简单文本格式逐行列出，便于后续处理。
- **阶段2（识别人员位置）**：调用视觉模型检测所有人员（包括被遮挡或截断者），同样输出归一化坐标的边界框，确保不遗漏任何在场个体。
- **阶段3（检测缺岗）**：先识别空置工位区域，再通过双重检验确认是否为真实缺岗——一是判断该区域是否位于设备拟合直线与图像底边之间（即工作区），二是验证其与最近人员的距离是否大于一个典型人体高度。
- **阶段4（拟合直线并判断身份）**：基于设备（优先显示器/键盘，其次办公桌）中心点进行线性回归拟合“工作台前沿线”；对每位人员在其框内采样多点，统计位于直线下方的比例，若超过阈值（50% 或 30%，依设备类型而定）则判定为工作人员，否则为顾客。
- **阶段5（绘制基础标注）**：在原图上绘制设备框（显示器紫色、办公桌绿色）、拟合直线（红色虚线）、人员框（工作人员蓝色、顾客橙色）及对应身份标签。
- **阶段6（添加缺岗标注）**：在已绘制图像基础上，用红色虚线框标出经双重检验确认的缺岗区域，并在上方添加“缺岗”文字标签。
- **阶段7（检测工作人员违规行为）**：对每位工作人员单独裁剪图像（并遮蔽其他人员以减少干扰），送入模型检测三类违规：玩手机、便服、睡觉；其中“玩手机”会进一步结合是否有对面顾客进行二次验证，避免误判。
- **阶段8（检测工作人员聊天行为）**：将所有工作人员区域组合裁剪成一张图，送入模型判断是否存在两人以上面对面交流、眼神接触等聊天特征，并返回具体的聊天人员对。
- **阶段9（绘制违规与聊天标签）**：在原图上仅绘制工作人员的蓝色框，并在其附近叠加违规行为标签（如“玩手机”“便服”）；同时用紫色连线连接聊天人员对，并在连线中点标注“聊天”。
- **最终（保存汇总结果）**：生成包含所有可视化标注的最终图像，并另存一份结构化文本报告，涵盖设备、人员身份、缺岗、违规、聊天等详细结果及各步骤的模型响应与判断过程，便于审计与复盘。

## 4.效果
### 4.1 效果1
识别了左上角的缺岗；但是对于个别顾客有漏识别
![demo5_结果_基础标注](https://github.com/user-attachments/assets/baf9d575-55d6-4c7f-8d04-16163cdd066b)

### 4.2 效果2
识别了违规行为-穿便服，但是对于打手机，图像描述中输出了打手机行为，但是没有输出坐标框，说明视觉模型的语言理解能力有欠缺。

使用qwen模型对该人员识别结果为“工作人员正坐在办公桌前，身体略微前倾，双手靠近面部，似乎正在使用座机电话。其右手持握座机话筒（可见话筒连接线），话筒未放置在座机底座上，而是被拿在手中贴近耳朵和嘴部，符合正在通话状态”
![demo14_结果_人员框和标签](https://github.com/user-attachments/assets/f03bfcb4-6a56-4dbf-b0a0-8966e4a96c47)

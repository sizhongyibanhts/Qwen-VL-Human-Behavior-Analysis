# 营业厅人员行为分析工具

## 1. 工具简介
business_hall_analysis：仅具备识别人员功能，可参考 https://zhuanlan.zhihu.com/p/1995169948964759155

### 1.1 原有方法
behavior_detection.py，基于多模态大模型的“文本描述→违规推导”流程，先整体分析工位/人员行为（输出自然语言描述），再基于描述文本推导违规行为及坐标，核心依赖大模型的自然语言理解与生成能力。

### 1.2 优化后方法
behavior_detection_sahi.py,参考目标检测中的sahi的处理思路，基于“单人员独立识别+坐标位置计算”的流程，先对每个人员独立提取特征与坐标，通过坐标运算判断位置关系，再结合规则判定违规，减少大模型主观判断依赖。

每个人员单独调用一次大模型，针对顾客与员工之间的互动、员工之间的互动，也分别截图调用大模型判断，避免模型在一整张图中识别单个人员行为，提升判断的准确度。

利用坐标进行位置关系判断（例如判断人员是否位于柜台内，进而判断身份）。

### 1.3 核心差异对比
| 维度                | 原有方法                          | 优化后方法                          |
|---------------------|-----------------------------------|-------------------------------------|
| 人员识别方式        | 整体描述所有人员，易遗漏/混淆     | 每个人员单独识别，特征精准无遗漏    |
| 位置关系判断        | 大模型直接主观判断（易幻觉）      | 坐标计算客观判定（如柜台内外、顾客位置） |
| 违规判定逻辑        | 基于文本描述推导（易失真）        | 基于坐标+规则判定（可溯源、无幻觉） |
| 抗幻觉能力          | 弱（依赖大模型文本生成稳定性）    | 强（坐标计算替代主观判断）          |

### 1.4 优化后方法的核心优势
1. **单人员独立识别**：避免多人员整体描述导致的特征混淆、信息遗漏，每个人员的行为/特征可精准追溯；
2. **坐标化位置计算**：用坐标运算替代大模型对“柜台内外/顾客位置”的主观判断，彻底解决位置关系判定的“幻觉问题”；
3. **规则化违规判定**：违规逻辑与大模型解耦，判定标准统一、结果可复现，降低对大模型生成质量的依赖。

优化后方法通过“单人员独立识别+坐标位置计算”的核心设计，解决了原有方法中“大模型文本幻觉、位置判断失真、人员特征混淆”的核心问题，检测结果的精准度、稳定性、可解释性均显著优于原有方法。

## 2.原有方法 核心流程

参见behavior_detection.py，大致流程如下：

```
输入图片 → 编码为Base64 → 分阶段分析 → 违规检测 → 可视化标注
```
- **阶段1（工位分析）**：调用多模态模型识别柜台内工位总数、每个工位位置/人员状态，输出文本描述并保存；
- **阶段2（人员行为分析）**：识别图片总人数、柜台位置，逐人分析身份/位置/衣着/手机使用/聊天/睡觉等行为，输出文本描述并保存；
- **阶段3（工位违规检测）**：基于工位描述，检测“岗位无人”违规，输出带坐标的JSON结果；
- **阶段3.5（人员描述简化）**：剔除顾客信息，仅保留工作人员的位置/身份/行为/潜在违规；
- **阶段4（人员违规检测）**：基于简化后的人员描述，检测“玩手机/聊天/睡觉/便服”违规，输出带坐标的JSON结果；
- **阶段5（可视化）**：读取违规JSON，在图片上绘制不同颜色的检测框+标签，完成标注。

## 3. 优化后方法 核心流程

参见behavior_detection_sahi.py，大致流程如下：

```
输入图片 → 编码为Base64 → 单人员独立识别 → 坐标化位置计算 → 违规判定 → 可视化标注
```
- - **阶段1（图片编码）**：将输入图片转换为Base64格式，为后续多模态模型调用做准备；
- - **阶段2（工位分析）**：调用多模态模型识别柜台内工位总数、每个工位位置/人员状态，输出文本描述并保存；
- - **阶段3（人员行为分析）**：识别图片总人数、柜台位置，逐人分析身份/位置/衣着/手机使用/聊天/睡觉等行为，输出文本描述并保存；
- - **阶段4（工位违规检测）**：基于工位描述，检测“岗位无人”违规，输出带坐标的JSON结果；
- - **阶段5（人员描述简化）**：剔除顾客信息，仅保留工作人员的位置/身份/行为/潜在违规；
- - **阶段6（人员违规检测）**：基于简化后的人员描述，检测“玩手机/聊天/睡觉/便服”违规，输出带坐标的JSON结果；
- - **阶段7（可视化）**：读取违规JSON，在图片上绘制不同颜色的检测框+标签，完成标注；
- - **阶段8（结果归档）**：保存标注后的图片、违规描述文本及JSON结果，完成整个检测流程。

## 4.效果
### 4.1 效果1
识别了左上角的缺岗；但是对于个别顾客有漏识别
![demo5_结果_基础标注](https://github.com/user-attachments/assets/baf9d575-55d6-4c7f-8d04-16163cdd066b)

### 4.2 效果2
识别了违规行为-穿便服，但是对于打手机，图像描述中输出了打手机行为，但是没有输出坐标框，说明视觉模型的语言理解能力有欠缺。

使用qwen模型对该人员识别结果为“工作人员正坐在办公桌前，身体略微前倾，双手靠近面部，似乎正在使用座机电话。其右手持握座机话筒（可见话筒连接线），话筒未放置在座机底座上，而是被拿在手中贴近耳朵和嘴部，符合正在通话状态”
![demo14_结果_人员框和标签](https://github.com/user-attachments/assets/f03bfcb4-6a56-4dbf-b0a0-8966e4a96c47)
